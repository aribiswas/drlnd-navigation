{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Control using Reinforcement Learning\n",
    "This project solves a navigation task for the Unity ML-Agents ***Banana*** environment. An agent is trained using the Double DQN algorithm to navigate and collect bananas in a large square world.\n",
    "\n",
    "For a detailed description of the environment, agent, and training algorithm, see ***Report.md***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "from agents import DQNAgent\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the environment\n",
    "There are prerequisites that need to be installed for running this project. Install the prerequisites as mentioned in the project README. After you have downloaded the platform-specific environment, specify the ***file_name*** variable as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Unity environment as per platform\n",
    "file_name = \"./Banana.app\"                         # Mac OSX\n",
    "#file_name = \"./Banana_Linux/Banana.x86\"            # Linux 32 bit\n",
    "#file_name = \"./Banana_Linux/Banana.x86_64\"         # Linux 64 bit\n",
    "#file_name = \"./Banana_Windows_x86/Banana.exe\"      # Windows 32 bit\n",
    "#file_name = \"./Banana_Windows_x86_64/Banana.exe\"   # Windows 64 bit\n",
    "\n",
    "# create environment\n",
    "env = UnityEnvironment(file_name)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reinforcement learning agent for this project is trained using the Double DQN algorithm. The implementation can be found in ***agents.py***. Specify the set of hyperparameters for training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFERSIZE = int(1e6)    # Experience buffer size\n",
    "GAMMA = 0.99             # Discount factor\n",
    "EPSILON = 0.95           # Epsilon parameter for exploration\n",
    "DECAY = 1e-5             # Epsilon decay rate\n",
    "EPMIN = 0.1              # Minimum value of epsilon\n",
    "MINIBATCHSIZE = 64       # Batch size for sampling from experience replay\n",
    "LEARNRATE = 2e-4         # Learn rate of Q network 2e-4\n",
    "TAU = 1e-2               # Target network update factor\n",
    "MAX_EPISODES = 5000      # Maximum number of training episodes  5000\n",
    "AVG_WINDOW = 100         # Window length for calculating score averages\n",
    "MAX_STEPS_PER_EPISODE = 1000    # Maximum agent steps per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the observations and action dimensions from the environment\n",
    "osize = len(env_info.vector_observations[0])\n",
    "asize = brain.vector_action_space_size\n",
    "seed = 0\n",
    "# create the DQN agent\n",
    "agent = DQNAgent(osize,asize,seed,BUFFERSIZE,GAMMA,EPSILON,DECAY,EPMIN,MINIBATCHSIZE,LEARNRATE,TAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n",
    "To train the agent run the code cell below. Training is computationally intensive and may take several minutes. To watch a pre-trained agent, skip to the next section.\n",
    "\n",
    "By default, the training will print metrics in the console. To change the verbosity, set the ***VERBOSE*** variable to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scores\n",
    "reward_log = []\n",
    "avg_log = []\n",
    "avg_window = collections.deque(maxlen=AVG_WINDOW)\n",
    "\n",
    "# verbosity\n",
    "VERBOSE = True\n",
    "\n",
    "# Train the agent\n",
    "for ep_count in range(1,MAX_EPISODES):\n",
    "\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    \n",
    "    ep_reward = 0\n",
    "    \n",
    "    for t in range(1,MAX_STEPS_PER_EPISODE):\n",
    "        # sample action from the current policy\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        # step the environment\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0] \n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        # step the agent\n",
    "        agent.step(state,action,reward,next_state,done)\n",
    "        \n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        \n",
    "        # terminate if done\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # print training progress\n",
    "    avg_window.append(ep_reward)\n",
    "    avg_reward = np.mean(avg_window)\n",
    "    avg_log.append(avg_reward)\n",
    "    reward_log.append(ep_reward)\n",
    "    if VERBOSE and (ep_count==1 or ep_count%1==0):\n",
    "        print('EP: {:4d} \\tEPR: {:4.4f} \\tAVR: {:4.4f} \\tEpsilon: {:.4f} \\tLoss: {:.4f}'.format(ep_count,ep_reward,avg_reward,agent.epsilon,agent.loss_log[ep_count]))\n",
    "    \n",
    "    # check if env is solved\n",
    "    if avg_reward >= 13:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(ep_count, avg_reward))\n",
    "        torch.save(agent.Q.state_dict(), 'checkpoint.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training results. The results will be saved in the ***results.png*** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score history\n",
    "plt.ion()\n",
    "fig, axarr = plt.subplots(2,1, figsize=(6,6), dpi=200)\n",
    "ax1 = axarr[0]\n",
    "ax1.set_title(\"Training Results\")\n",
    "ax1.set_xlabel(\"Episodes\")\n",
    "ax1.set_ylabel(\"Average Reward\")\n",
    "ax1.set_xlim([0, ep_count+20])\n",
    "ax1.set_ylim([0, 20])\n",
    "ax1.plot(range(1,ep_count+1),avg_log)\n",
    "\n",
    "# plot loss\n",
    "ax2 = axarr[1]\n",
    "ax2.set_xlabel(\"Steps\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_xlim([0, agent.stepcount+20])\n",
    "ax2.plot(agent.loss_log)\n",
    "\n",
    "fig.tight_layout(pad=1.0)\n",
    "plt.show()\n",
    "fig.savefig('results.png',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample training results is shown below. In this training, the agent achieved **average reward of +13 in 793 episodes**.\n",
    "\n",
    "<img src=\"./results.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a smart agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will load the network weights from a pre-trained agent and use them to simulate the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.Q.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "# number of simulations\n",
    "NUM_SIMS = 3\n",
    "\n",
    "for i in range(NUM_SIMS):\n",
    "    \n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    \n",
    "    for t in range(1,MAX_STEPS_PER_EPISODE):\n",
    "        # sample action\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        # step the environment\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0] \n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
